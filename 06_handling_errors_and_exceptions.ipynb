{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428a1693-6c4d-423f-a176-13c4345c885c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7f40bfcd1d05:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Handling errors and Exceptions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f78d0de7b80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Handling errors and Exceptions\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config('spark.jars', '/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.20.jar')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ee70c4-9260-4bcf-82f1-acff3f39ce06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the kafka_df to read from kafka\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "    .option(\"subscribe\", \"device-data\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60fe3f43-bbb6-43dc-93db-f0ee921193e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined logic for handling the error records\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import from_json, col, expr, explode, current_timestamp, lit, size\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "\n",
    "def flatten_data(df):\n",
    "    \n",
    "    # Convert binary to string value column\n",
    "    kafka_json_df = df.withColumn(\"value\", expr(\"cast(value as string)\"))\n",
    "    \n",
    "    # Define Schema\n",
    "    json_schema = (\n",
    "        StructType(\n",
    "        [StructField('customerId', StringType(), True), \n",
    "        StructField('data', StructType(\n",
    "            [StructField('devices', \n",
    "                         ArrayType(StructType([ \n",
    "                            StructField('deviceId', StringType(), True), \n",
    "                            StructField('measure', StringType(), True), \n",
    "                            StructField('status', StringType(), True), \n",
    "                            StructField('temperature', LongType(), True)\n",
    "                        ]), True), True)\n",
    "            ]), True), \n",
    "        StructField('eventId', StringType(), True), \n",
    "        StructField('eventOffset', LongType(), True), \n",
    "        StructField('eventPublisher', StringType(), True), \n",
    "        StructField('eventTime', StringType(), True)\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Expand JSON from Value column using Schema\n",
    "    json_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema))\n",
    "    \n",
    "    # Filter out for error data\n",
    "    error_df = json_df.select(\"key\", \"value\").withColumn(\"eventtimestamp\",lit(current_timestamp())) \\\n",
    "        .where(\"values_json.customerId is null or size(values_json.data.devices) = 0\")\n",
    "    \n",
    "    # Filter out correct flattened data\n",
    "    streaming_df = json_df.where(\"values_json.customerId is not null and size(values_json.data.devices) > 0\") \\\n",
    "        .selectExpr(\"values_json.*\")\n",
    "    \n",
    "    # Explode the correct flattened data\n",
    "    exploded_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "    \n",
    "    # Flatten data\n",
    "    flattened_df = (\n",
    "    exploded_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    "    )\n",
    "\n",
    "    # Return both Flattened & Error Dataframe\n",
    "    return flattened_df, error_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc18c5b-ec95-4719-adff-5b9f349e625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write the dataframe to JDBC (Postgres)\n",
    "\n",
    "def postgres(df, table_name):\n",
    "    (\n",
    "\tdf.write\n",
    "\t.mode(\"append\")\n",
    "\t.format(\"jdbc\")\n",
    "\t.option(\"driver\", \"org.postgresql.Driver\")\n",
    "\t.option(\"url\", \"jdbc:postgresql://postgres-db-1:5432/sqlpad\")\n",
    "\t.option(\"dbtable\", table_name)\n",
    "\t.option(\"user\", \"sqlpad\")\n",
    "\t.option(\"password\", \"sqlpad\")\n",
    "\t.save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aef5000-5ac0-46bd-90b1-3a13aec2e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Error and Exception and write to JDBC \n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def device_data_output(kafka_df, batch_id):\n",
    "    print(\"Batch id:\" + str(batch_id))\n",
    "    try:\n",
    "        # Get the Flattened and Error Dataframe\n",
    "        flattened_df, error_df_raw = flatten_data(kafka_df)\n",
    "\n",
    "        # Add the batchid column in Error Dataframe\n",
    "        error_df = error_df_raw.withColumn(\"batchid\", lit(batch_id))\n",
    "\n",
    "        # Write Flattened Dataframe to JDBC\n",
    "        postgres(flattened_df, \"device_data\")\n",
    "\n",
    "        # Write Error Datafram to JDBC\n",
    "        postgres(error_df, \"device_data_error\")\n",
    "\n",
    "        # Display both Dataframes for confirmation\n",
    "        flattened_df.show()\n",
    "        error_df.show()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        kafka_df.write.format(\"parquet\").mode(\"append\").save(\"data/output/device_data_error.parquet\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a2511-13c6-4edf-972e-51b4c0a0f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch id:4\n",
      "An error occurred while calling o393.save.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:315)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:51)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:223)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:465)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:264)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:122)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:118)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:660)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.net.UnknownHostException: postgres-db-1\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:231)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:95)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:98)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:213)\n",
      "\t... 79 more\n",
      "\n",
      "Batch id:5\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "|customerId|eventId|eventOffset|eventPublisher|eventTime|deviceId|measure|status|temperature|\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "\n",
      "+----+----------+--------------------+-------+\n",
      "| key|     value|      eventtimestamp|batchid|\n",
      "+----+----------+--------------------+-------+\n",
      "|null|error data|2024-03-19 08:40:...|      5|\n",
      "+----+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running foreachBatch\n",
    "\n",
    "(kafka_df\n",
    " .writeStream\n",
    " .foreachBatch(device_data_output)\n",
    " .trigger(processingTime='10 seconds')\n",
    " .option(\"checkpointLocation\", \"checkpoint_dir_kafka\")\n",
    " .start()\n",
    " .awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf71619-2113-4c85-a84d-4fff55847c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
