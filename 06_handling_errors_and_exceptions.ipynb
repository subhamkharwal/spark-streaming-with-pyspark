{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae8cbfd",
   "metadata": {},
   "source": [
    "Below code will download the required JAR file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "wget https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.20/postgresql-42.2.20.jar\n",
    "mkdir -p /home/jovyan/.ivy2/jars/\n",
    "mv postgresql-42.2.20.jar /home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.20.jar\n",
    "ls -ltr /home/jovyan/.ivy2/jars/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e572e",
   "metadata": {},
   "source": [
    "\n",
    "Run the below SQL Query to create the required tables in Postgres DB. Make sure to select connection as Postgres from the dropdown in SQLPAD. You can access SQLPAD to query Postgres DB using `https://localhost:3000`. Login credentials for SQLPAD `USER: admin@sqlpad.com PASSWORD: admin`\n",
    "```sql\n",
    "CREATE TABLE public.device_data (\n",
    "\tcustomerid varchar,\n",
    "\teventid varchar,\n",
    "\teventoffset varchar,\n",
    "\teventpublisher varchar,\n",
    "\teventtime varchar,\n",
    "\tdeviceid varchar,\n",
    "\tmeasure varchar,\n",
    "\tstatus varchar,\n",
    "\ttemperature varchar\n",
    "\n",
    ");\n",
    "\n",
    "CREATE TABLE public.device_data_error (\n",
    "\tkey varchar,\n",
    "\tvalue varchar,\n",
    "\teventtimestamp timestamp,\n",
    "\tbatchid int\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428a1693-6c4d-423f-a176-13c4345c885c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7f40bfcd1d05:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Handling errors and Exceptions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f78d0de7b80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Handling errors and Exceptions\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config('spark.jars', '/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.20.jar')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ee70c4-9260-4bcf-82f1-acff3f39ce06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the kafka_df to read from kafka\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "    .option(\"subscribe\", \"device-data\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60fe3f43-bbb6-43dc-93db-f0ee921193e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined logic for handling the error records\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import from_json, col, expr, explode, current_timestamp, lit, size\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "\n",
    "def flatten_data(df):\n",
    "    \n",
    "    # Convert binary to string value column\n",
    "    kafka_json_df = df.withColumn(\"value\", expr(\"cast(value as string)\"))\n",
    "    \n",
    "    # Define Schema\n",
    "    json_schema = (\n",
    "        StructType(\n",
    "        [StructField('customerId', StringType(), True), \n",
    "        StructField('data', StructType(\n",
    "            [StructField('devices', \n",
    "                         ArrayType(StructType([ \n",
    "                            StructField('deviceId', StringType(), True), \n",
    "                            StructField('measure', StringType(), True), \n",
    "                            StructField('status', StringType(), True), \n",
    "                            StructField('temperature', LongType(), True)\n",
    "                        ]), True), True)\n",
    "            ]), True), \n",
    "        StructField('eventId', StringType(), True), \n",
    "        StructField('eventOffset', LongType(), True), \n",
    "        StructField('eventPublisher', StringType(), True), \n",
    "        StructField('eventTime', StringType(), True)\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Expand JSON from Value column using Schema\n",
    "    json_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema))\n",
    "    \n",
    "    # Filter out for error data\n",
    "    error_df = json_df.select(\"key\", \"value\").withColumn(\"eventtimestamp\",lit(current_timestamp())) \\\n",
    "        .where(\"values_json.customerId is null or size(values_json.data.devices) = 0\")\n",
    "    \n",
    "    # Filter out correct flattened data\n",
    "    streaming_df = json_df.where(\"values_json.customerId is not null and size(values_json.data.devices) > 0\") \\\n",
    "        .selectExpr(\"values_json.*\")\n",
    "    \n",
    "    # Explode the correct flattened data\n",
    "    exploded_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "    \n",
    "    # Flatten data\n",
    "    flattened_df = (\n",
    "    exploded_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    "    )\n",
    "\n",
    "    # Return both Flattened & Error Dataframe\n",
    "    return flattened_df, error_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc18c5b-ec95-4719-adff-5b9f349e625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write the dataframe to JDBC (Postgres)\n",
    "\n",
    "def postgres(df, table_name):\n",
    "    (\n",
    "\tdf.write\n",
    "\t.mode(\"append\")\n",
    "\t.format(\"jdbc\")\n",
    "\t.option(\"driver\", \"org.postgresql.Driver\")\n",
    "\t.option(\"url\", \"jdbc:postgresql://postgres-db:5432/sqlpad\")\n",
    "\t.option(\"dbtable\", table_name)\n",
    "\t.option(\"user\", \"sqlpad\")\n",
    "\t.option(\"password\", \"sqlpad\")\n",
    "\t.save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aef5000-5ac0-46bd-90b1-3a13aec2e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Error and Exception and write to JDBC \n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def device_data_output(kafka_df, batch_id):\n",
    "    print(\"Batch id:\" + str(batch_id))\n",
    "    try:\n",
    "        # Get the Flattened and Error Dataframe\n",
    "        flattened_df, error_df_raw = flatten_data(kafka_df)\n",
    "\n",
    "        # Add the batchid column in Error Dataframe\n",
    "        error_df = error_df_raw.withColumn(\"batchid\", lit(batch_id))\n",
    "\n",
    "        # Write Flattened Dataframe to JDBC\n",
    "        postgres(flattened_df, \"device_data\")\n",
    "\n",
    "        # Write Error Datafram to JDBC\n",
    "        postgres(error_df, \"device_data_error\")\n",
    "\n",
    "        # Display both Dataframes for confirmation\n",
    "        flattened_df.show()\n",
    "        error_df.show()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        kafka_df.write.format(\"parquet\").mode(\"append\").save(\"data/output/device_data_error.parquet\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a2511-13c6-4edf-972e-51b4c0a0f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch id:4\n",
      "An error occurred while calling o393.save.\n",
      ": org.postgresql.util.PSQLException: The connection attempt failed.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:315)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:51)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:223)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:465)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:264)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:122)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:118)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:660)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.net.UnknownHostException: postgres-db-1\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:231)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:95)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:98)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:213)\n",
      "\t... 79 more\n",
      "\n",
      "Batch id:5\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "|customerId|eventId|eventOffset|eventPublisher|eventTime|deviceId|measure|status|temperature|\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "\n",
      "+----+----------+--------------------+-------+\n",
      "| key|     value|      eventtimestamp|batchid|\n",
      "+----+----------+--------------------+-------+\n",
      "|null|error data|2024-03-19 08:40:...|      5|\n",
      "+----+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running foreachBatch\n",
    "\n",
    "(kafka_df\n",
    " .writeStream\n",
    " .foreachBatch(device_data_output)\n",
    " .trigger(processingTime='10 seconds')\n",
    " .option(\"checkpointLocation\", \"checkpoint_dir_kafka\")\n",
    " .start()\n",
    " .awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf71619-2113-4c85-a84d-4fff55847c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
